---
layout: post
title: Redis Performance - Does key length matter?
date: 2012-11-15 08:00:41.000000000 -06:00
categories:
- Node.js
tags:
- coffeescript
- node.js
- NoSQL
- redis
status: publish
type: post
published: true
meta:
  _edit_last: '25054220'
  _publicize_pending: '1'
  _wpas_done_881585: '1'
  _publicize_done_external: a:1:{s:8:"facebook";a:1:{i:1471181839;b:1;}}
  publicize_twitter_user: adamnengland
  _wpas_done_881586: '1'
  _wpas_done_881584: '1'
  _wpas_skip_881585: '1'
  _wpas_skip_881586: '1'
  _wpas_skip_881584: '1'
author:
  login: adamnengland
  email: adam.n.england@gmail.com
  display_name: adamnengland
  first_name: ''
  last_name: ''
excerpt: !ruby/object:Hpricot::Doc
  options: {}
---
<p>I'm currently building a project using <a href="http://redis.io/">redis</a> as a high performance cache in a node.js application (using the excellent <a href="https://github.com/mranney/node_redis">node_redis</a>). My key values will be fairly large ( between 512b and 1kb). The Redis documentation doesn't specifically warn against keys of this size, but it still seems appropriate to do a benchmark, and see how Redis reacts to large keys (and whether or not 1k is really a <strong>large</strong> key, or just par for the course).</p>
<p><span style="text-decoration:underline;"><strong>Test Script</strong></span><strong> </strong>(<a href="https://gist.github.com/4076309">source</a>)</p>
<p>Basically, we insert 1000 records into redis, each with a 10,000 character value. After the writes are all complete, we read each key back from redis.</p>
<p>{% highlight javascript %}<br />
redis = require "redis"</p>
<p>randomString = (length) -&amp;gt;<br />
  chars = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'<br />
  result = ""<br />
  i = length</p>
<p>  while i > 0<br />
    result += chars[Math.round(Math.random() * (chars.length - 1))]<br />
    --i<br />
  result</p>
<p>writeTest = (keyLength) -&amp;gt;<br />
	console.log "1000 set statements for #{keyLength} character keys"<br />
	keys = []<br />
	for x in [1..1000]<br />
		keys.push randomString(keyLength)<br />
	startTime = new Date().getTime()<br />
	for x in keys<br />
		client.set x, randomString(10000)<br />
	client.quit -&amp;gt;<br />
		console.log "1000 keys inserted in #{new Date().getTime() - startTime} ms"<br />
		readTest(keys)</p>
<p>readTest = (keys) -&amp;gt;<br />
	client = redis.createClient()<br />
	startTime = new Date().getTime()<br />
	for x in keys<br />
		client.get x<br />
	client.quit -&amp;gt;<br />
		console.log "1000 keys retreived in #{new Date().getTime() - startTime} ms"</p>
<p>client = redis.createClient()</p>
<p>client.flushdb -&amp;gt;<br />
	writeTest(20000)</p>
<p>{% endhighlight %}</p>
<p>This test was performed for key lengths of 10, 100, 500, 1000, 2500, 5000, 7500, 10,000, and 20,000 characters. Three runs of each were performed to avoid any fluke results. Without further ado, the results.</p>
<p><span style="text-decoration:underline;"><strong>Write Performance (in ms)</strong></span></p>
<table style="border:1px solid black;">
<tbody>
<tr style="border:1px solid black;">
<th>Key Length</th>
<th>Run 1</th>
<th>Run 2</th>
<th>Run 3</th>
</tr>
<tr style="border:1px solid black;">
<td>10</td>
<td>1235</td>
<td>1216</td>
<td>1259</td>
<td></td>
</tr>
<tr style="border:1px solid black;">
<td>100</td>
<td>1231</td>
<td>1242</td>
<td>1223</td>
</tr>
<tr style="border:1px solid black;">
<td>500</td>
<td>1283</td>
<td>1240</td>
<td>1270</td>
</tr>
<tr style="border:1px solid black;">
<td>1000</td>
<td>1277</td>
<td>1317</td>
<td>1345</td>
</tr>
<tr style="border:1px solid black;">
<td>2500</td>
<td>1318</td>
<td>1279</td>
<td>1294</td>
</tr>
<tr style="border:1px solid black;">
<td>5000</td>
<td>1376</td>
<td>1391</td>
<td>1386</td>
</tr>
<tr style="border:1px solid black;">
<td>7500</td>
<td>1223</td>
<td>1204</td>
<td>1265</td>
</tr>
<tr style="border:1px solid black;">
<td>10000</td>
<td>1220</td>
<td>1252</td>
<td>1235</td>
</tr>
<tr style="border:1px solid black;">
<td>20000</td>
<td>2065</td>
<td>2014</td>
<td>2016</td>
</tr>
</tbody>
</table>
<p><span style="text-decoration:underline;"><strong>Read Performance (in ms)</strong></span></p>
<table style="border:1px solid black;">
<tbody>
<tr style="border:1px solid black;">
<th>Key Length</th>
<th>Run 1</th>
<th>Run 2</th>
<th>Run 3</th>
</tr>
<tr style="border:1px solid black;">
<td>10</td>
<td>43</td>
<td>41</td>
<td>51</td>
</tr>
<tr style="border:1px solid black;">
<td>100</td>
<td>45</td>
<td>45</td>
<td>43</td>
</tr>
<tr style="border:1px solid black;">
<td>500</td>
<td>60</td>
<td>54</td>
<td>58</td>
</tr>
<tr style="border:1px solid black;">
<td>1000</td>
<td>69</td>
<td>73</td>
<td>79</td>
</tr>
<tr style="border:1px solid black;">
<td>2500</td>
<td>97</td>
<td>101</td>
<td>102</td>
</tr>
<tr style="border:1px solid black;">
<td>5000</td>
<td>113</td>
<td>114</td>
<td>110</td>
</tr>
<tr style="border:1px solid black;">
<td>7500</td>
<td>134</td>
<td>133</td>
<td>136</td>
</tr>
<tr style="border:1px solid black;">
<td>10000</td>
<td>147</td>
<td>156</td>
<td>151</td>
</tr>
<tr style="border:1px solid black;">
<td>20000</td>
<td>244</td>
<td>234</td>
<td>241</td>
</tr>
</tbody>
</table>
<p>Not surprisingly, as the key length increases, times do increase.  However, write times are relatively unaffected by key length, while read times seem to be impacted more.   To put it in perspective:</p>
<ul>
<li>Key length 10 - an average write takes 1.24ms, an average read takes 0.045ms</li>
<li>Key length 10,000 - an average write takes 1.24ms, an average read takes 0.15ms</li>
</ul>
<p>Whether or not this is significant is really up to you, however, for my purposes, it seems like an insignificant difference.  At the end of the day, redis is a fast and flexible tool for caching data.</p>
